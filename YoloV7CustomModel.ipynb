{"cells":[{"cell_type":"code","source":"!git clone https://github.com/WongKinYiu/yolov7","metadata":{"cell_id":"d26b8176af7b402abe614478b17bcf9b","source_hash":"ec6b608f","execution_start":1682193440365,"execution_millis":3268,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Cloning into 'yolov7'...\nremote: Enumerating objects: 1139, done.\u001b[K\nremote: Total 1139 (delta 0), reused 0 (delta 0), pack-reused 1139\u001b[K\nReceiving objects: 100% (1139/1139), 70.41 MiB | 40.69 MiB/s, done.\nResolving deltas: 100% (488/488), done.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!wget \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\"","metadata":{"cell_id":"7a51848d18e845188f14374f04af8231","source_hash":"fbf58a88","execution_start":1682352877788,"execution_millis":528,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"--2023-04-24 16:14:37--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230424T161438Z&X-Amz-Expires=300&X-Amz-Signature=572e3b8b57ccc0efcb657be1fa380fa7205a31eefa0df4fb8349527979b8206a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream [following]\n--2023-04-24 16:14:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230424T161438Z&X-Amz-Expires=300&X-Amz-Signature=572e3b8b57ccc0efcb657be1fa380fa7205a31eefa0df4fb8349527979b8206a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12639769 (12M) [application/octet-stream]\nSaving to: ‘yolov7-tiny.pt’\n\nyolov7-tiny.pt      100%[===================>]  12.05M  --.-KB/s    in 0.05s   \n\n2023-04-24 16:14:38 (264 MB/s) - ‘yolov7-tiny.pt’ saved [12639769/12639769]\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pdf = p_df.values\nrdf = r_df.values\n\nprint(pdf[0][1])","metadata":{"cell_id":"e1fe3c7998f5426082b83c8a9431085c","source_hash":"8c67e20","execution_start":1682176570753,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72caf8.jpg\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pydicom\n\ndef convert(path):\n    image = pydicom.dcmread(path)\n    image = image.pixel_array.astype(float)\n    rescaled = (np.maximum(image, 0)/image.max()) * 255 # Float Pixels\n    final_image = np.uint8(rescaled)\n    final_image = Image.fromarray(final_image)\n    return final_image","metadata":{"cell_id":"6db888af35414eb7a7bca3cbb6889b35","source_hash":"87f97573","execution_start":1682176537780,"execution_millis":68,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def center_format_rescale(center_x, center_y, width, height, current_width, current_height, new_width, new_height):\n    # Calculate the scaling factors\n    scale_x = new_width / current_width\n    scale_y = new_height / current_height\n    \n    # Calculate the new center coordinates\n    new_center_x = center_x * scale_x\n    new_center_y = center_y * scale_y\n\n    # Caculate new width and height\n    new_width = width * scale_x\n    new_height = height * scale_y\n\n    return new_center_x, new_center_y, new_width, new_height","metadata":{"cell_id":"9063bb8b6d5045fcadbebe848b8df06f","source_hash":"3166c591","execution_start":1682174281227,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('actualObject2.csv')\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf","metadata":{"cell_id":"8871c60e2e754873a37e3deadab2e70d","source_hash":"64a17028","execution_start":1682810180443,"execution_millis":269,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":8955,"columns":[{"name":"Absolute Path","dtype":"object","stats":{"unique_count":8805,"nan_count":0,"categories":[{"name":"/datasets/acm-drive-data/ACM DATA/yolo/covid_pneumonia/c7925ab50eb0.jpg","count":6},{"name":"/datasets/acm-drive-data/ACM DATA/yolo/covid_pneumonia/4bb94cd7f2f4.jpg","count":5},{"name":"8803 others","count":8944}]}},{"name":"Relative Path","dtype":"object","stats":{"unique_count":8805,"nan_count":0,"categories":[{"name":"c7925ab50eb0.jpg","count":6},{"name":"4bb94cd7f2f4.jpg","count":5},{"name":"8803 others","count":8944}]}},{"name":"Bounding Boxes","dtype":"object","stats":{"unique_count":6430,"nan_count":0,"categories":[{"name":"[{'x': 86.49735452054794, 'y': 52.574812976022564, 'w': 112.51031452054794, 'h': 173.07106753173485}, {'x': 246.1273512328767, 'y': 53.36690820874471, 'w': 119.96291726027397, 'h': 159.01151503526094}]","count":360},{"name":"[{'x': 57.929420917431194, 'y': 35.03340451127819, 'w': 75.35094458715596, 'h': 115.32649142857143}, {'x': 164.83758385321102, 'y': 35.56121984962406, 'w': 80.34213724770643, 'h': 105.95786105263157}]","count":284},{"name":"6428 others","count":8311}]}},{"name":"Output","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Pneumonia","count":6012},{"name":"COVID","count":2929},{"name":"Cardiomegaly","count":14}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Absolute Path":"pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72caf8.jpg","Relative Path":"149d826e-7078-4b2c-888a-d8811c72caf8.jpg","Bounding Boxes":"[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'height': 50.5}]","Output":"Pneumonia","_deepnote_index_column":"0"},{"Absolute Path":"pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg","Relative Path":"b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg","Bounding Boxes":"[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'height': 26.5}]","Output":"Pneumonia","_deepnote_index_column":"1"},{"Absolute Path":"pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg","Relative Path":"3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg","Bounding Boxes":"[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'height': 55.0}]","Output":"Pneumonia","_deepnote_index_column":"2"},{"Absolute Path":"pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg","Relative Path":"d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg","Bounding Boxes":"[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'height': 70.0}]","Output":"Pneumonia","_deepnote_index_column":"3"},{"Absolute Path":"pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba73b50.jpg","Relative Path":"9acc3015-cffb-414a-a8b1-87555ba73b50.jpg","Bounding Boxes":"[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'height': 63.75}]","Output":"Pneumonia","_deepnote_index_column":"4"},{"Absolute Path":"pneumoniaPics/f7250628-0db8-4baa-adbf-fd694cbd3b7d.jpg","Relative Path":"f7250628-0db8-4baa-adbf-fd694cbd3b7d.jpg","Bounding Boxes":"[{'x': 47.5, 'y': 105.0, 'width': 48.75, 'height': 106.25}]","Output":"Pneumonia","_deepnote_index_column":"5"},{"Absolute Path":"pneumoniaPics/564eac4e-bec9-4cd6-94cb-cc6ba78983b9.jpg","Relative Path":"564eac4e-bec9-4cd6-94cb-cc6ba78983b9.jpg","Bounding Boxes":"[{'x': 147.0, 'y': 92.5, 'width': 61.25, 'height': 78.0}]","Output":"Pneumonia","_deepnote_index_column":"6"},{"Absolute Path":"pneumoniaPics/71a96026-101c-4da6-a576-60cd288b9733.jpg","Relative Path":"71a96026-101c-4da6-a576-60cd288b9733.jpg","Bounding Boxes":"[{'x': 78.25, 'y': 67.5, 'width': 23.75, 'height': 56.0}]","Output":"Pneumonia","_deepnote_index_column":"7"},{"Absolute Path":"pneumoniaPics/bf7bc9a6-d454-4edd-8752-9273448a3b10.jpg","Relative Path":"bf7bc9a6-d454-4edd-8752-9273448a3b10.jpg","Bounding Boxes":"[{'x': 158.0, 'y': 83.75, 'width': 74.0, 'height': 142.75}]","Output":"Pneumonia","_deepnote_index_column":"8"},{"Absolute Path":"pneumoniaPics/fcf5cd90-1a5d-4e45-925e-ff82dcbdc0ad.jpg","Relative Path":"fcf5cd90-1a5d-4e45-925e-ff82dcbdc0ad.jpg","Bounding Boxes":"[{'x': 62.5, 'y': 128.75, 'width': 45.5, 'height': 41.5}]","Output":"Pneumonia","_deepnote_index_column":"9"}]},"text/plain":"                                          Absolute Path  \\\n0     pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72...   \n1     pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f...   \n2     pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1...   \n3     pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3...   \n4     pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba7...   \n...                                                 ...   \n8950  /datasets/acm-drive-data/ACM DATA/Part8/000166...   \n8951  /datasets/acm-drive-data/ACM DATA/Part8/000183...   \n8952  /datasets/acm-drive-data/ACM DATA/Part8/000175...   \n8953  /datasets/acm-drive-data/ACM DATA/Part8/000181...   \n8954  /datasets/acm-drive-data/ACM DATA/Part8/000175...   \n\n                                 Relative Path  \\\n0     149d826e-7078-4b2c-888a-d8811c72caf8.jpg   \n1     b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg   \n2     3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg   \n3     d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg   \n4     9acc3015-cffb-414a-a8b1-87555ba73b50.jpg   \n...                                        ...   \n8950                          00016606_000.png   \n8951                          00018387_030.png   \n8952                          00017524_028.png   \n8953                          00018187_034.png   \n8954                          00017514_008.png   \n\n                                         Bounding Boxes        Output  \n0     [{'x': 55.25, 'y': 107.0, 'width': 44.75, 'hei...     Pneumonia  \n1     [{'x': 152.5, 'y': 65.5, 'width': 28.5, 'heigh...     Pneumonia  \n2     [{'x': 56.5, 'y': 137.5, 'width': 59.5, 'heigh...     Pneumonia  \n3     [{'x': 131.5, 'y': 59.0, 'width': 77.75, 'heig...     Pneumonia  \n4     [{'x': 141.25, 'y': 113.75, 'width': 72.25, 'h...     Pneumonia  \n...                                                 ...           ...  \n8950  [{'x': 91.83492063492075, 'y': 106.19259259259...  Cardiomegaly  \n8951  [{'x': 97.22033898305075, 'y': 89.568363512976...  Cardiomegaly  \n8952  [{'x': 62.84867724867725, 'y': 108.90158730158...  Cardiomegaly  \n8953  [{'x': 120.271186440678, 'y': 99.186440677966,...  Cardiomegaly  \n8954  [{'x': 86.68783068783075, 'y': 111.33968253968...  Cardiomegaly  \n\n[8955 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Absolute Path</th>\n      <th>Relative Path</th>\n      <th>Bounding Boxes</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72...</td>\n      <td>149d826e-7078-4b2c-888a-d8811c72caf8.jpg</td>\n      <td>[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'hei...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f...</td>\n      <td>b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg</td>\n      <td>[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'heigh...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1...</td>\n      <td>3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg</td>\n      <td>[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'heigh...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3...</td>\n      <td>d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg</td>\n      <td>[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'heig...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba7...</td>\n      <td>9acc3015-cffb-414a-a8b1-87555ba73b50.jpg</td>\n      <td>[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'h...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8950</th>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000166...</td>\n      <td>00016606_000.png</td>\n      <td>[{'x': 91.83492063492075, 'y': 106.19259259259...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8951</th>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000183...</td>\n      <td>00018387_030.png</td>\n      <td>[{'x': 97.22033898305075, 'y': 89.568363512976...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8952</th>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000175...</td>\n      <td>00017524_028.png</td>\n      <td>[{'x': 62.84867724867725, 'y': 108.90158730158...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8953</th>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000181...</td>\n      <td>00018187_034.png</td>\n      <td>[{'x': 120.271186440678, 'y': 99.186440677966,...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8954</th>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000175...</td>\n      <td>00017514_008.png</td>\n      <td>[{'x': 86.68783068783075, 'y': 111.33968253968...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n  </tbody>\n</table>\n<p>8955 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[['Absolute Path', 'Relative Path', 'Bounding Boxes']]\ny = df['Output']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, stratify=y_train)","metadata":{"cell_id":"fc4e3bc0ced840e98e5cbd75700513fd","source_hash":"94f8afe9","execution_start":1682810184253,"execution_millis":1280,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"X_train, X_val, X_test = X_train.values, X_val.values, X_test.values","metadata":{"cell_id":"1dc66a1adc9146fe84fddfbba210e4f9","source_hash":"1332d02a","execution_start":1682810185709,"execution_millis":32,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"for i in range(len(X_train)):\n    image = None\n    if os.path.exists(X_train[i][0]):\n        image = Image.open(X_train[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    # Create a directory if it doesn't exist\n    if not os.path.exists('yolov7/data/Train/images'):\n        os.makedirs('yolov7/data/Train/images')\n\n    width, height = image.size[0], image.size[1]\n    new_image = image.resize((640, 640)) \n    grey = ImageOps.grayscale(new_image)\n    filename = 'yolov7/data/Train/images/' + X_train[i][1]\n    \n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"7a75328d82e7400dbc539db514e0e632","source_hash":"5d01d3fd","execution_start":1682431511423,"execution_millis":847,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"[Errno 122] Disk quota exceeded: 'yolov7/data/Train/images/1260a8ac7750.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn [10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov7/data/Train/images/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[i][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the image to the 'images' directory\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mgrey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/PIL/Image.py:2317\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2320\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n","\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded: 'yolov7/data/Train/images/1260a8ac7750.jpg'"]}],"execution_count":null},{"cell_type":"code","source":"for i in range(len(X_val)):\n    image = None\n    if os.path.exists(X_val[i][0]):\n        image = Image.open(X_val[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    # Create a directory if it doesn't exist\n    if not os.path.exists('yolov7/data/Val/images'):\n        os.makedirs('yolov7/data/Val/images')\n\n    width, height = image.size[0], image.size[1]\n    new_image = image.resize((640, 640)) \n    grey = ImageOps.grayscale(new_image)\n    filename = 'yolov7/data/Val/images/' + X_val[i][1]\n    \n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"d799e0f0243d44c8852a68671541ef93","source_hash":"7f36e2dc","execution_start":1682431470120,"execution_millis":6703,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, ImageOps\nimport os\n\n# Open the image file\n\nfor i in range(len(X_test)):\n    image = None\n    if os.path.exists(X_test[i][0]):\n        image = Image.open(X_test[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    # Create a directory if it doesn't exist\n    if not os.path.exists('yolov7/data/Test/images'):\n        os.makedirs('yolov7/data/Test/images')\n\n    width, height = image.size[0], image.size[1]\n    new_image = image.resize((640, 640)) \n    grey = ImageOps.grayscale(new_image)\n    filename = 'yolov7/data/Test/images/' + X_test[i][1]\n\n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"6796e8a8468644648418fcad07f8141a","source_hash":"ebf36f21","execution_start":1682431447047,"execution_millis":1846,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def center_format_rescale(center_x, center_y, width, height, current_width, current_height, new_width, new_height):\n    # Calculate the scaling factors\n    scale_x = new_width / current_width\n    scale_y = new_height / current_height\n    \n    # Calculate the new center coordinates\n    new_center_x = center_x * scale_x\n    new_center_y = center_y * scale_y\n\n    # Caculate new width and height\n    new_width = width * scale_x\n    new_height = height * scale_y\n\n    return new_center_x, new_center_y, new_width, new_height","metadata":{"cell_id":"7bd9e5fd585d42ea9d22df1e83d84408","source_hash":"3166c591","execution_start":1682396795078,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport ast\nflag = False\n\nclasses = {\"Pneumonia\": 0,\n            \"COVID\": 1,\n            \"Cardiomegaly\": 2}\n\n# Define function to convert bounding box data to YOLOv7 format\ndef to_yolov7(bbox, output, width, height):\n    out = []\n    bbox = list(eval(bbox))\n    res = []\n    print(bbox)\n    x, y, w, h = None, None, None, None\n    for i in range(len(bbox)):\n        x, y = bbox[i]['x'], bbox[i]['y']\n        if 'w' in bbox[i]:\n            w = bbox[i]['w']\n        if 'h' in bbox[i]:\n            h = bbox[i]['h']\n        if 'width' in bbox[i]:\n            w = bbox[i]['width']\n        if 'height'in bbox[i]:\n            h = bbox[i]['height']\n\n        x, y, w, h = center_format_rescale(x, y, w, h, 256, 256, 640, 640)\n        # Convert coordinates to YOLOv7 format\n        x_center = x / width\n        y_center = y / height\n        box_width = w / width\n        box_height = h / height\n        res.append((x_center, y_center, box_width, box_height))\n\n    label = classes[output]\n\n    # Add bounding box to output list\n    string = ''\n    for i in range(len(res)):\n        string += f\"{label} {res[i][0]} {res[i][1]} {res[i][2]} {res[i][3]}\\n\"\n        \n    return string\n\npath = ['yolov7/data/Test/labels', 'yolov7/data/Val/labels', 'yolov7/data/Train/labels']\n\nfor i in range(10):\n    # Example list of dictionaries containing bounding box data\n    bbox_data = df[['Bounding Boxes', 'Output']].values[i]\n    # Convert bounding box data to YOLOv7 format\n    yolo_data = to_yolov7(bbox_data[0], bbox_data[1], 640, 640)\n\n    text = X_test[i][1][:-4] + \".txt\"\n    completeName = os.path.join(path[0], text)\n\n    if not os.path.exists(path[0]):\n        os.makedirs(path[0])\n        \n    with open(completeName, \"w\") as file:\n        file.write(yolo_data)\n\nfor i in range(10):\n    # Example list of dictionaries containing bounding box data\n    bbox_data = df[['Bounding Boxes', 'Output']].values[i]\n    # Convert bounding box data to YOLOv7 format\n    yolo_data = to_yolov7(bbox_data[0], bbox_data[1], 640, 640)\n    \n    text = X_val[i][1][:-4] + \".txt\"\n    completeName = os.path.join(path[1], text)\n    \n    if not os.path.exists(path[1]):\n        os.makedirs(path[1])\n        \n    with open(completeName, \"w\") as file:\n        file.write(yolo_data)\n\nfor i in range(50):\n    # Example list of dictionaries containing bounding box data\n    bbox_data = df[['Bounding Boxes', 'Output']].values[i]\n    # Convert bounding box data to YOLOv7 format\n    yolo_data = to_yolov7(bbox_data[0], bbox_data[1], 640, 640)\n    \n    text = X_train[i][1][:-4] + \".txt\"\n    completeName = os.path.join(path[2], text)\n    \n    if not os.path.exists(path[2]):\n        os.makedirs(path[2])\n        \n    with open(completeName, \"w\") as file:\n        file.write(yolo_data)","metadata":{"cell_id":"52a423310cdc4349b9163e021c866b62","source_hash":"6ce5a0d1","execution_start":1682396828308,"execution_millis":239,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'height': 50.5}]\n[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'height': 26.5}]\n[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'height': 55.0}]\n[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'height': 70.0}]\n[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'height': 63.75}]\n[{'x': 47.5, 'y': 105.0, 'width': 48.75, 'height': 106.25}]\n[{'x': 147.0, 'y': 92.5, 'width': 61.25, 'height': 78.0}]\n[{'x': 78.25, 'y': 67.5, 'width': 23.75, 'height': 56.0}]\n[{'x': 158.0, 'y': 83.75, 'width': 74.0, 'height': 142.75}]\n[{'x': 62.5, 'y': 128.75, 'width': 45.5, 'height': 41.5}]\n[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'height': 50.5}]\n[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'height': 26.5}]\n[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'height': 55.0}]\n[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'height': 70.0}]\n[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'height': 63.75}]\n[{'x': 47.5, 'y': 105.0, 'width': 48.75, 'height': 106.25}]\n[{'x': 147.0, 'y': 92.5, 'width': 61.25, 'height': 78.0}]\n[{'x': 78.25, 'y': 67.5, 'width': 23.75, 'height': 56.0}]\n[{'x': 158.0, 'y': 83.75, 'width': 74.0, 'height': 142.75}]\n[{'x': 62.5, 'y': 128.75, 'width': 45.5, 'height': 41.5}]\n[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'height': 50.5}]\n[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'height': 26.5}]\n[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'height': 55.0}]\n[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'height': 70.0}]\n[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'height': 63.75}]\n[{'x': 47.5, 'y': 105.0, 'width': 48.75, 'height': 106.25}]\n[{'x': 147.0, 'y': 92.5, 'width': 61.25, 'height': 78.0}]\n[{'x': 78.25, 'y': 67.5, 'width': 23.75, 'height': 56.0}]\n[{'x': 158.0, 'y': 83.75, 'width': 74.0, 'height': 142.75}]\n[{'x': 62.5, 'y': 128.75, 'width': 45.5, 'height': 41.5}]\n[{'x': 45.0, 'y': 125.5, 'width': 44.0, 'height': 66.25}]\n[{'x': 165.5, 'y': 124.5, 'width': 50.0, 'height': 60.75}]\n[{'x': 69.5, 'y': 134.5, 'width': 45.75, 'height': 14.25}]\n[{'x': 65.75, 'y': 46.25, 'width': 58.75, 'height': 129.0}]\n[{'x': 166.75, 'y': 113.5, 'width': 64.0, 'height': 89.75}]\n[{'x': 84.75, 'y': 71.0, 'width': 40.5, 'height': 49.75}]\n[{'x': 61.0, 'y': 82.75, 'width': 30.25, 'height': 76.5}]\n[{'x': 76.0, 'y': 70.0, 'width': 27.75, 'height': 26.25}]\n[{'x': 134.25, 'y': 112.5, 'width': 38.0, 'height': 32.25}]\n[{'x': 52.25, 'y': 58.0, 'width': 70.5, 'height': 143.0}]\n[{'x': 54.5, 'y': 60.25, 'width': 35.0, 'height': 127.25}]\n[{'x': 122.75, 'y': 112.0, 'width': 72.5, 'height': 91.75}]\n[{'x': 53.75, 'y': 49.0, 'width': 50.25, 'height': 162.75}]\n[{'x': 53.5, 'y': 85.0, 'width': 17.0, 'height': 15.5}]\n[{'x': 37.0, 'y': 151.5, 'width': 51.5, 'height': 31.25}]\n[{'x': 65.5, 'y': 75.5, 'width': 44.5, 'height': 111.75}]\n[{'x': 157.0, 'y': 52.5, 'width': 59.0, 'height': 90.75}]\n[{'x': 42.0, 'y': 50.75, 'width': 68.0, 'height': 160.5}]\n[{'x': 46.0, 'y': 124.75, 'width': 27.25, 'height': 38.75}]\n[{'x': 53.25, 'y': 45.25, 'width': 60.75, 'height': 115.5}]\n[{'x': 18.25, 'y': 64.0, 'width': 72.25, 'height': 104.0}]\n[{'x': 90.0, 'y': 63.25, 'width': 45.0, 'height': 48.0}]\n[{'x': 138.5, 'y': 92.25, 'width': 44.75, 'height': 47.0}]\n[{'x': 59.5, 'y': 88.5, 'width': 49.5, 'height': 82.0}]\n[{'x': 126.0, 'y': 141.5, 'width': 57.0, 'height': 48.25}]\n[{'x': 47.75, 'y': 72.5, 'width': 58.75, 'height': 70.5}]\n[{'x': 47.75, 'y': 68.5, 'width': 56.0, 'height': 120.25}]\n[{'x': 74.25, 'y': 117.5, 'width': 31.75, 'height': 40.0}]\n[{'x': 29.5, 'y': 141.5, 'width': 62.0, 'height': 52.75}]\n[{'x': 172.25, 'y': 136.25, 'width': 22.75, 'height': 33.5}]\n[{'x': 172.25, 'y': 97.0, 'width': 48.75, 'height': 49.25}]\n[{'x': 107.25, 'y': 37.0, 'width': 42.0, 'height': 62.75}]\n[{'x': 161.0, 'y': 91.5, 'width': 43.5, 'height': 57.25}]\n[{'x': 145.0, 'y': 97.25, 'width': 76.75, 'height': 85.5}]\n[{'x': 165.0, 'y': 145.25, 'width': 50.0, 'height': 43.0}]\n[{'x': 131.25, 'y': 130.0, 'width': 68.5, 'height': 84.5}]\n[{'x': 52.5, 'y': 130.5, 'width': 32.0, 'height': 15.75}]\n[{'x': 128.0, 'y': 96.5, 'width': 66.5, 'height': 80.5}]\n[{'x': 50.5, 'y': 125.75, 'width': 42.25, 'height': 38.0}]\n[{'x': 135.0, 'y': 16.5, 'width': 56.25, 'height': 49.75}]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def center_format_rescale(center_x, center_y, width, height, current_width, current_height, new_width, new_height):\n    # Calculate the scaling factors\n    scale_x = new_width / current_width\n    scale_y = new_height / current_height\n    \n    # Calculate the new center coordinates\n    new_center_x = center_x * scale_x\n    new_center_y = center_y * scale_y\n\n    # Caculate new width and height\n    new_width = width * scale_x\n    new_height = height * scale_y\n\n    return new_center_x, new_center_y, new_width, new_height","metadata":{"cell_id":"5797a52339114ac5988946e1b37b5912","source_hash":"3166c591","execution_start":1682810132869,"execution_millis":68,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import albumentations as A        # For data augmentation\nimport cv2                        # For loading images\nimport matplotlib.pyplot as plt   # For plotting images\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclasses = {\"Pneumonia\": 0,\n            \"COVID\": 1,\n            \"Cardiomegaly\": 2}\n\n#X_train = X_train.values\n#y_train = y_train.values\n#print(y_train)\nfor i in range(len(X_train)):\n    labels = []\n    bbox = []\n    dic_list = X_train[i][2]\n    dic = list(eval(dic_list))\n    for j in range(len(dic)):\n        d = dic[j]\n        x, y, w, h = d['x'], d['y'], None, None\n        if 'width' in d:\n            w = d['width']\n        if 'w' in d:\n            w = d['w']\n        if 'height' in d:\n            h = d['height']\n        if 'h' in d:\n            h = d['h']\n\n        x, y, w, h = center_format_rescale(x, y, w, h, 256, 256, 640, 640)\n        bbox.append([x, y, w, h])\n        #print(bbox)\n        #print(y_train[0])\n        labels.append(y_train[i])\n\n    transform1 = A.Compose([\n        A.HorizontalFlip(p=1.0)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    transform2 = A.Compose([\n        A.Rotate(limit=45, p=1.0)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    transform3 = A.Compose([\n        A.RandomCrop(width=640, height=640)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    image = Image.open(X_train[i][0])\n    image.save('normal.png')\n    image = np.array(image)\n    transformed1 = transform1(image=image, bboxes=bbox, class_labels=labels)\n    transformed2 = transform2(image=image, bboxes=bbox, class_labels=labels)\n    transformed3 = transform3(image=image, bboxes=bbox, class_labels=labels)\n\n    img1 = Image.fromarray(transformed1['image'])\n    img2 = Image.fromarray(transformed2['image'])\n    img3 = Image.fromarray(transformed3['image'])\n\n    rgb1 = img1.convert('RGB')\n    rgb2 = img2.convert('RGB')\n    rgb3 = img3.convert('RGB')\n\n    input_dir = 'yolov7/data/Train/images/'\n    filename1 = os.path.join(input_dir, f'Horizontal{X_train[i][1]}')\n    filename2 = os.path.join(input_dir, f'Rotate{X_train[i][1]}')\n    filename3 = os.path.join(input_dir, f'Crop{X_train[i][1]}')\n    img1 = Image.fromarray(transformed1['image'])\n    img2 = Image.fromarray(transformed2['image'])\n    img3 = Image.fromarray(transformed3['image'])\n\n    if not os.path.exists(input_dir):\n        os.makedirs(input_dir)\n\n    #print(filename1)\n    img1.save(filename1)\n    img2.save(filename2)\n    img3.save(filename3)\n\n\n    # Add bounding box to output list\n    b1, b2, b3 = transformed1['bboxes'], transformed2['bboxes'], transformed3['bboxes']\n    string1 = ''\n    string2 = ''\n    string3 = ''\n    for l in range(len(b1)):\n        string1 += f\"{classes[labels[l]]} {b1[l]} {b1[l]} {b1[l]}\\n\"\n    for l in range(len(b2)):\n        string2 += f\"{classes[labels[l]]} {b2[l]} {b2[l]} {b2[l]}\\n\"\n    for l in range(len(b3)):\n        string3 += f\"{classes[labels[l]]} {b3[l]} {b3[l]} {b3[l]}\\n\"\n\n    filename = 'yolov7/data/Train/labels/' \n    if not os.path.exists(filename):\n        os.makedirs(filename)\n\n    filename1 = filename + f'Horizontal{X_train[i][1][:-4]}.txt'\n    with open(filename1, \"w\") as file:\n        file.write(string1)\n    \n    filename2 = filename + f'Rotate{X_train[i][1][:-4]}.txt'\n    with open(filename2, \"w\") as file:\n        file.write(string2)\n    \n    filename3 = filename + f'Crop{X_train[i][1][:-4]}.txt'\n    with open(filename3, \"w\") as file:\n        file.write(string3)\n\n\n","metadata":{"cell_id":"e7a14849e25541a491072e7aea5a0b8b","source_hash":"43b143bb","execution_start":1682810941479,"execution_millis":2034,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!python yolov7/train.py --workers 4 --device cpu --batch-size 8 --epochs 10 --img 640 640 --data yolov7/data/custom_data.yaml --hyp yolov7/data/hyp.scratch.custom.yaml --cfg yolov7/cfg/training/yolov7-custom.yaml --name yolov7-custom --weights yolov7.pt\n#!python yolov7/train.py --batch 8 --cfg cfg/training/yolov7.yaml --epochs 10 --data ./custom_data.yaml --weights 'yolov7.pt' --device 0 --entity 'yolov7' --project 'yolov7' --name 'run1'","metadata":{"cell_id":"2fc4351d451b4b52bdf476d779383eba","source_hash":"5590cc94","execution_start":1682397654895,"execution_millis":400118,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"YOLOR 🚀 v0.1-122-g3b41c2c torch 1.12.1+cu102 CPU\n\nNamespace(weights='yolov7.pt', cfg='yolov7/cfg/training/yolov7-custom.yaml', data='yolov7/data/custom_data.yaml', hyp='yolov7/data/hyp.scratch.custom.yaml', epochs=10, batch_size=8, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='cpu', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=4, project='runs/train', entity=None, name='yolov7-custom', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7-custom17', total_batch_size=8)\n\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n2023-04-25 04:40:58.561900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-25 04:40:58.673877: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-25 04:40:58.678263: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /root/venv/lib/python3.9/site-packages/cv2/../../lib64:\n2023-04-25 04:40:58.678285: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-04-25 04:40:58.701141: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-04-25 04:40:59.972794: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /root/venv/lib/python3.9/site-packages/cv2/../../lib64:\n2023-04-25 04:40:59.972854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /root/venv/lib/python3.9/site-packages/cv2/../../lib64:\n2023-04-25 04:40:59.972865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n 12                -1  1         0  models.common.MP                        []                            \n 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n 25                -1  1         0  models.common.MP                        []                            \n 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n 38                -1  1         0  models.common.MP                        []                            \n 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n 76                -1  1         0  models.common.MP                        []                            \n 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n 89                -1  1         0  models.common.MP                        []                            \n 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n105   [102, 103, 104]  1     44944  models.yolo.IDetect                     [3, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\nModel Summary: 415 layers, 37207344 parameters, 37207344 gradients\n\nTransferred 552/566 items from yolov7.pt\nScaled weight_decay = 0.0005\nOptimizer groups: 95 .bias, 95 conv.weight, 98 other\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/work/yolov7/data/Train/labels.cache' images and labels... 50 f\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning '/work/yolov7/data/val/labels.cache' images and labels... 10 found\u001b[0m\n\n\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.28, Best Possible Recall (BPR) = 1.0000\n/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\nImage sizes 640 train, 640 test\nUsing 4 dataloader workers\nLogging results to runs/train/yolov7-custom17\nStarting training for 10 epochs...\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n  0%|                                                     | 0/7 [00:00<?, ?it/s]/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n       0/9        0G   0.05599   0.01608   0.01896   0.09103         5       640\n               Class      Images      Labels           P           R      mAP@.5/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10      0.0118         0.1     0.00279     0.00109\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       1/9        0G    0.0716    0.0163   0.02138    0.1093         5       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10      0.0116         0.3     0.00434    0.000906\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       2/9        0G   0.06214   0.01486   0.02126   0.09827         6       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10       0.017         0.1     0.00429    0.000629\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       3/9        0G   0.06527   0.01411    0.0223    0.1017         2       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10           0           0           0           0           0\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       4/9        0G   0.06271   0.01334   0.02046   0.09652         4       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10       0.347         0.1      0.0475      0.0095\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       5/9        0G   0.05357   0.01241   0.01948   0.08546         1       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10      0.0362         0.1     0.00554     0.00103\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       6/9        0G   0.06511   0.01236   0.02029   0.09776         2       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10       0.486      0.0971      0.0505      0.0102\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       7/9        0G   0.06207   0.01127   0.01863   0.09198         1       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10      0.0232         0.1     0.00257    0.000539\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n       8/9        0G   0.06032   0.01118   0.01751     0.089         2       640\n               Class      Images      Labels           P           R      mAP@.5\n                 all          10          10        0.11         0.1      0.0133     0.00133\nTraceback (most recent call last):\n  File \"/work/yolov7/train.py\", line 616, in <module>\n    train(hyp, opt, device, tb_writer)\n  File \"/work/yolov7/train.py\", line 474, in train\n    torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/serialization.py\", line 380, in save\n    return\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/serialization.py\", line 214, in __exit__\n    self.file_like.close()\nOSError: [Errno 122] Disk quota exceeded\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5eba63a0-2067-4a72-82b1-ae21917b1281' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"41e6f9eba4124b508ecda895636597f2","deepnote_execution_queue":[]}}