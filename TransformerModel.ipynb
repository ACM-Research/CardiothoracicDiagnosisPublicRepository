{"cells":[{"cell_type":"code","source":"!git clone https://github.com/rwightman/pytorch-image-models.git","metadata":{"cell_id":"322b5e14959449f0bdfa4ff4bb43a6f6","source_hash":"fb12f937","execution_start":1682707490796,"execution_millis":193,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"fatal: destination path 'pytorch-image-models' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"!pip install transformers","metadata":{"cell_id":"719869d90ee346a2a282fb576728a970","source_hash":"34487cb0","execution_start":1682481186472,"execution_millis":1323,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement transormers (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transormers\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!conda install timm","metadata":{"cell_id":"a459a16c32cf4445b5458aa11192ed3a","source_hash":"7dd35b8d","execution_start":1682705886005,"execution_millis":145100,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n\nResolvePackageNotFound: \n  - conda==23.1.0\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!rm -r transformers\n!git clone -b upload_detr_no_timm https://github.com/nielsrogge/transformers.git\n!cd transformers \n!pip install -q ./transformers\n!pip install -q pytorch-lightning\n!pip install pycocotools","metadata":{"cell_id":"e2f273533c3943c1a45e50e34693179f","source_hash":"b920bba8","output_cleared":false,"execution_start":1682481204535,"execution_millis":94297,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Cloning into 'transformers'...\nremote: Enumerating objects: 183114, done.\u001b[K\nremote: Counting objects: 100% (2682/2682), done.\u001b[K\nremote: Compressing objects: 100% (770/770), done.\u001b[K\nremote: Total 183114 (delta 1823), reused 2348 (delta 1601), pack-reused 180432\u001b[K\nReceiving objects: 100% (183114/183114), 155.47 MiB | 34.77 MiB/s, done.\nResolving deltas: 100% (136432/136432), done.\nChecking out files: 100% (3087/3087), done.\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting pycocotools\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pycocotools) (1.23.4)\nRequirement already satisfied: matplotlib>=2.1.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pycocotools) (3.6.0)\nRequirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (9.2.0)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (1.0.5)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (4.37.4)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp39-cp39-linux_x86_64.whl size=359846 sha256=33a437b418717d63364fa186eba4d09c41f0c1e187a6c4e9ef179d14fdf89fc4\n  Stored in directory: /root/.cache/pip/wheels/2f/58/25/e78f1f766e904a9071266661d20d0bc6644df86bcd160aba11\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.6\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install efficientnet_pytorch==0.7.1","metadata":{"cell_id":"0300e8c9c8264bdfb07aa70eb2bbfe13","source_hash":"1b508442","execution_start":1682547289990,"execution_millis":3508,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /shared-libs/python3.9/py/lib/python3.9/site-packages (from efficientnet_pytorch==0.7.1) (1.12.1)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torch->efficientnet_pytorch==0.7.1) (4.4.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=0456e7beba4bed6d2022f8eb722d88662edf0a533195b9982a8244f56d2488e2\n  Stored in directory: /root/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('actualObject2.csv')\ndf","metadata":{"cell_id":"45c3e0e3cf144135a7e603b1f5713f80","source_hash":"15904261","output_cleared":false,"execution_start":1682547272036,"execution_millis":66,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":5,"row_count":8955,"columns":[{"name":"Unnamed: 0","dtype":"int64","stats":{"unique_count":8955,"nan_count":0,"min":"0","max":"8954","histogram":[{"bin_start":0,"bin_end":895.4,"count":896},{"bin_start":895.4,"bin_end":1790.8,"count":895},{"bin_start":1790.8,"bin_end":2686.2,"count":896},{"bin_start":2686.2,"bin_end":3581.6,"count":895},{"bin_start":3581.6,"bin_end":4477,"count":895},{"bin_start":4477,"bin_end":5372.4,"count":896},{"bin_start":5372.4,"bin_end":6267.8,"count":895},{"bin_start":6267.8,"bin_end":7163.2,"count":896},{"bin_start":7163.2,"bin_end":8058.599999999999,"count":895},{"bin_start":8058.599999999999,"bin_end":8954,"count":896}]}},{"name":"Absolute Path","dtype":"object","stats":{"unique_count":8805,"nan_count":0,"categories":[{"name":"/datasets/acm-drive-data/ACM DATA/yolo/covid_pneumonia/c7925ab50eb0.jpg","count":6},{"name":"/datasets/acm-drive-data/ACM DATA/yolo/covid_pneumonia/4bb94cd7f2f4.jpg","count":5},{"name":"8803 others","count":8944}]}},{"name":"Relative Path","dtype":"object","stats":{"unique_count":8805,"nan_count":0,"categories":[{"name":"c7925ab50eb0.jpg","count":6},{"name":"4bb94cd7f2f4.jpg","count":5},{"name":"8803 others","count":8944}]}},{"name":"Bounding Boxes","dtype":"object","stats":{"unique_count":6430,"nan_count":0,"categories":[{"name":"[{'x': 86.49735452054794, 'y': 52.574812976022564, 'w': 112.51031452054794, 'h': 173.07106753173485}, {'x': 246.1273512328767, 'y': 53.36690820874471, 'w': 119.96291726027397, 'h': 159.01151503526094}]","count":360},{"name":"[{'x': 57.929420917431194, 'y': 35.03340451127819, 'w': 75.35094458715596, 'h': 115.32649142857143}, {'x': 164.83758385321102, 'y': 35.56121984962406, 'w': 80.34213724770643, 'h': 105.95786105263157}]","count":284},{"name":"6428 others","count":8311}]}},{"name":"Output","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Pneumonia","count":6012},{"name":"COVID","count":2929},{"name":"Cardiomegaly","count":14}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Unnamed: 0":"0","Absolute Path":"pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72caf8.jpg","Relative Path":"149d826e-7078-4b2c-888a-d8811c72caf8.jpg","Bounding Boxes":"[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'height': 50.5}]","Output":"Pneumonia","_deepnote_index_column":"0"},{"Unnamed: 0":"1","Absolute Path":"pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg","Relative Path":"b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg","Bounding Boxes":"[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'height': 26.5}]","Output":"Pneumonia","_deepnote_index_column":"1"},{"Unnamed: 0":"2","Absolute Path":"pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg","Relative Path":"3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg","Bounding Boxes":"[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'height': 55.0}]","Output":"Pneumonia","_deepnote_index_column":"2"},{"Unnamed: 0":"3","Absolute Path":"pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg","Relative Path":"d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg","Bounding Boxes":"[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'height': 70.0}]","Output":"Pneumonia","_deepnote_index_column":"3"},{"Unnamed: 0":"4","Absolute Path":"pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba73b50.jpg","Relative Path":"9acc3015-cffb-414a-a8b1-87555ba73b50.jpg","Bounding Boxes":"[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'height': 63.75}]","Output":"Pneumonia","_deepnote_index_column":"4"},{"Unnamed: 0":"5","Absolute Path":"pneumoniaPics/f7250628-0db8-4baa-adbf-fd694cbd3b7d.jpg","Relative Path":"f7250628-0db8-4baa-adbf-fd694cbd3b7d.jpg","Bounding Boxes":"[{'x': 47.5, 'y': 105.0, 'width': 48.75, 'height': 106.25}]","Output":"Pneumonia","_deepnote_index_column":"5"},{"Unnamed: 0":"6","Absolute Path":"pneumoniaPics/564eac4e-bec9-4cd6-94cb-cc6ba78983b9.jpg","Relative Path":"564eac4e-bec9-4cd6-94cb-cc6ba78983b9.jpg","Bounding Boxes":"[{'x': 147.0, 'y': 92.5, 'width': 61.25, 'height': 78.0}]","Output":"Pneumonia","_deepnote_index_column":"6"},{"Unnamed: 0":"7","Absolute Path":"pneumoniaPics/71a96026-101c-4da6-a576-60cd288b9733.jpg","Relative Path":"71a96026-101c-4da6-a576-60cd288b9733.jpg","Bounding Boxes":"[{'x': 78.25, 'y': 67.5, 'width': 23.75, 'height': 56.0}]","Output":"Pneumonia","_deepnote_index_column":"7"},{"Unnamed: 0":"8","Absolute Path":"pneumoniaPics/bf7bc9a6-d454-4edd-8752-9273448a3b10.jpg","Relative Path":"bf7bc9a6-d454-4edd-8752-9273448a3b10.jpg","Bounding Boxes":"[{'x': 158.0, 'y': 83.75, 'width': 74.0, 'height': 142.75}]","Output":"Pneumonia","_deepnote_index_column":"8"},{"Unnamed: 0":"9","Absolute Path":"pneumoniaPics/fcf5cd90-1a5d-4e45-925e-ff82dcbdc0ad.jpg","Relative Path":"fcf5cd90-1a5d-4e45-925e-ff82dcbdc0ad.jpg","Bounding Boxes":"[{'x': 62.5, 'y': 128.75, 'width': 45.5, 'height': 41.5}]","Output":"Pneumonia","_deepnote_index_column":"9"}]},"text/plain":"      Unnamed: 0                                      Absolute Path  \\\n0              0  pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72...   \n1              1  pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f...   \n2              2  pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1...   \n3              3  pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3...   \n4              4  pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba7...   \n...          ...                                                ...   \n8950        8950  /datasets/acm-drive-data/ACM DATA/Part8/000166...   \n8951        8951  /datasets/acm-drive-data/ACM DATA/Part8/000183...   \n8952        8952  /datasets/acm-drive-data/ACM DATA/Part8/000175...   \n8953        8953  /datasets/acm-drive-data/ACM DATA/Part8/000181...   \n8954        8954  /datasets/acm-drive-data/ACM DATA/Part8/000175...   \n\n                                 Relative Path  \\\n0     149d826e-7078-4b2c-888a-d8811c72caf8.jpg   \n1     b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg   \n2     3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg   \n3     d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg   \n4     9acc3015-cffb-414a-a8b1-87555ba73b50.jpg   \n...                                        ...   \n8950                          00016606_000.png   \n8951                          00018387_030.png   \n8952                          00017524_028.png   \n8953                          00018187_034.png   \n8954                          00017514_008.png   \n\n                                         Bounding Boxes        Output  \n0     [{'x': 55.25, 'y': 107.0, 'width': 44.75, 'hei...     Pneumonia  \n1     [{'x': 152.5, 'y': 65.5, 'width': 28.5, 'heigh...     Pneumonia  \n2     [{'x': 56.5, 'y': 137.5, 'width': 59.5, 'heigh...     Pneumonia  \n3     [{'x': 131.5, 'y': 59.0, 'width': 77.75, 'heig...     Pneumonia  \n4     [{'x': 141.25, 'y': 113.75, 'width': 72.25, 'h...     Pneumonia  \n...                                                 ...           ...  \n8950  [{'x': 91.83492063492075, 'y': 106.19259259259...  Cardiomegaly  \n8951  [{'x': 97.22033898305075, 'y': 89.568363512976...  Cardiomegaly  \n8952  [{'x': 62.84867724867725, 'y': 108.90158730158...  Cardiomegaly  \n8953  [{'x': 120.271186440678, 'y': 99.186440677966,...  Cardiomegaly  \n8954  [{'x': 86.68783068783075, 'y': 111.33968253968...  Cardiomegaly  \n\n[8955 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Absolute Path</th>\n      <th>Relative Path</th>\n      <th>Bounding Boxes</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>pneumoniaPics/149d826e-7078-4b2c-888a-d8811c72...</td>\n      <td>149d826e-7078-4b2c-888a-d8811c72caf8.jpg</td>\n      <td>[{'x': 55.25, 'y': 107.0, 'width': 44.75, 'hei...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>pneumoniaPics/b19a9422-a790-4a43-b59f-65a3bf0f...</td>\n      <td>b19a9422-a790-4a43-b59f-65a3bf0f16be.jpg</td>\n      <td>[{'x': 152.5, 'y': 65.5, 'width': 28.5, 'heigh...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>pneumoniaPics/3d4dfd10-61c7-4305-816f-fde544d1...</td>\n      <td>3d4dfd10-61c7-4305-816f-fde544d1f3a3.jpg</td>\n      <td>[{'x': 56.5, 'y': 137.5, 'width': 59.5, 'heigh...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>pneumoniaPics/d4e0b27e-79ad-4af1-bd49-6c7feaf3...</td>\n      <td>d4e0b27e-79ad-4af1-bd49-6c7feaf3f84c.jpg</td>\n      <td>[{'x': 131.5, 'y': 59.0, 'width': 77.75, 'heig...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>pneumoniaPics/9acc3015-cffb-414a-a8b1-87555ba7...</td>\n      <td>9acc3015-cffb-414a-a8b1-87555ba73b50.jpg</td>\n      <td>[{'x': 141.25, 'y': 113.75, 'width': 72.25, 'h...</td>\n      <td>Pneumonia</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8950</th>\n      <td>8950</td>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000166...</td>\n      <td>00016606_000.png</td>\n      <td>[{'x': 91.83492063492075, 'y': 106.19259259259...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8951</th>\n      <td>8951</td>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000183...</td>\n      <td>00018387_030.png</td>\n      <td>[{'x': 97.22033898305075, 'y': 89.568363512976...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8952</th>\n      <td>8952</td>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000175...</td>\n      <td>00017524_028.png</td>\n      <td>[{'x': 62.84867724867725, 'y': 108.90158730158...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8953</th>\n      <td>8953</td>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000181...</td>\n      <td>00018187_034.png</td>\n      <td>[{'x': 120.271186440678, 'y': 99.186440677966,...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n    <tr>\n      <th>8954</th>\n      <td>8954</td>\n      <td>/datasets/acm-drive-data/ACM DATA/Part8/000175...</td>\n      <td>00017514_008.png</td>\n      <td>[{'x': 86.68783068783075, 'y': 111.33968253968...</td>\n      <td>Cardiomegaly</td>\n    </tr>\n  </tbody>\n</table>\n<p>8955 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[['Absolute Path', 'Relative Path', 'Bounding Boxes', 'Output']]\ny = df['Output']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\nX_train, X_test = X_train.values, X_test.values","metadata":{"cell_id":"722b174f445a46d59a0f427da5c6f3fe","source_hash":"17aefcb7","output_cleared":false,"execution_start":1682547276211,"execution_millis":474,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, ImageOps\nimport os\n\n# Open the image file\n\nfor i in range(len(X_test)):\n    image = None\n    if os.path.exists(X_test[i][0]):\n        image = Image.open(X_test[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    new_image = image.resize((256, 256))\n    grey = ImageOps.grayscale(new_image)\n      \n    # Create a directory if it doesn't exist\n    if not os.path.exists('DATA/Test'):\n        os.makedirs('DATA/Test')\n\n    filename = 'DATA/Test/' + X_test[i][1]\n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"568d8d9456e44f7290ddadc7ebf2c206","source_hash":"34263a0b","output_cleared":false,"execution_start":1682468045024,"execution_millis":19591,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(X_val)):\n    image = None\n    if os.path.exists(X_val[i][0]):\n        image = Image.open(X_val[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    new_image = image.resize((256, 256))\n    grey = ImageOps.grayscale(new_image) \n\n    # Create a directory if it doesn't exist\n    if not os.path.exists('DATA/Test'):\n        os.makedirs('DATA/Test')\n\n    filename = 'DATA/Test/' + X_val[i][1]\n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"725ca30b40584f80bd28e546bead64d2","source_hash":"6be04e35","output_cleared":false,"execution_start":1682464002235,"execution_millis":196155,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(X_train)):\n    image = None\n    if os.path.exists(X_train[i][0]):\n        image = Image.open(X_train[i][0])\n    else:\n        df.drop(i, axis=0, inplace=True)\n        continue\n\n    new_image = image.resize((256, 256))\n    grey = ImageOps.grayscale(new_image)\n      \n    # Create a directory if it doesn't exist\n    if not os.path.exists('DATA/Train'):\n        os.makedirs('DATA/Train')\n\n    filename = 'DATA/Train/' + X_train[i][1]\n    # Save the image to the 'images' directory\n    grey.save(filename)","metadata":{"cell_id":"457012596a9644f38488dfe68f3fae0c","source_hash":"ef649530","output_cleared":false,"execution_start":1682468064619,"execution_millis":77386,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"cell_id":"61389bf5869242eead1b2bbc2448a8a8","source_hash":"d36abc86","output_cleared":true,"execution_start":1682347496497,"execution_millis":5143,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resnet50-Efficient Combination\nclass CombinedModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # initial layer\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n        # ResNet50 branch\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1280)\n\n        # EfficientNetB0 branch\n        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n\n    def forward(self, x):\n        # pass input through initial layer\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.maxpool1(out)\n\n        out = self.conv3(out)\n        out = self.relu(out)\n        out = self.conv4(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.maxpool2(out)\n\n        # pass input through ResNet50 branch\n        out1 = self.resnet(out)\n\n        # pass input through EfficientNetB0 branch\n        out2 = self.efficientnet(out)\n\n        # concatenate outputs along feature dimension\n        out = torch.cat((out1, out2), dim=1)\n\n        return out","metadata":{"cell_id":"e7eed2dcdbd74436816f580c14ae4a55","source_hash":"5ef8eed","output_cleared":true,"execution_start":1682347590445,"execution_millis":3331,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DETRdemo(torch.nn.Module):\n    \"\"\"\n    Demo DETR implementation.\n\n    Demo implementation of DETR in minimal number of lines, with the\n    following differences wrt DETR in the paper:\n    * learned positional encoding (instead of sine)\n    * positional encoding is passed at input (instead of attention)\n    * fc bbox predictor (instead of MLP)\n    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n    Only batch size 1 supported.\n    \"\"\"\n    def __init__(self, num_classes, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6):\n        super().__init__()\n\n        # create ResNet-50 backbone\n        self.backbone = CombinedModel()\n        # create conversion layer\n        # self.conv = nn.Conv2d(1280, hidden_dim, 1)\n\n        # create a default PyTorch transformer\n        self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n\n        # prediction heads, one extra class for predicting non-empty slots\n        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n        self.linear_bbox = nn.Linear(hidden_dim, 4)\n\n        # output positional encodings (object queries)\n        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n\n        # spatial positional encodings\n        # note that in baseline DETR we use sine positional encodings\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n\n    def forward(self, inputs):\n        # propagate inputs through ResNet-50 up to avg-pool layer\n        # x = self.backbone.conv1(inputs)\n        # x = self.backbone.bn1(x)\n        # x = self.backbone.relu(x)\n        # x = self.backbone.maxpool(x)\n\n        # x = self.backbone.layer1(x)\n        # x = self.backbone.layer2(x)\n        # x = self.backbone.layer3(x)\n        # x = self.backbone.layer4(x)\n\n        x = self.backbone.forward(inputs)\n        x = self.forward(x)\n        # convert from 2048 to 256 feature planes for the transformer\n        h = self.conv(x)\n\n        # x = self.backbone(inputs)\n        # h = self.conv(x)\n\n        # construct positional encodings\n        H, W = h.shape[-2:]\n        pos = torch.cat([\n            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n        ], dim=-1).flatten(0, 1).unsqueeze(1)\n\n        # propagate through the transformer\n        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n        \n        # finally project transformer outputs to class labels and bounding boxes\n        return {'pred_logits': self.linear_class(h), \n                'pred_boxes': self.linear_bbox(h).sigmoid()}","metadata":{"cell_id":"aefe806a47984be69320f95df20d23ed","source_hash":"c6d10caf","output_cleared":true,"execution_start":1682347594038,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"traindf = pd.DataFrame(X_train,columns = ['Absolute Path','Relative Path','Bounding Boxes', 'Output'])","metadata":{"cell_id":"2cda0dadc9b1400d81154ad7903fb214","source_hash":"4f0e398e","execution_start":1682468141934,"execution_millis":100,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"28656"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"testdf = pd.DataFrame(X_test,columns = ['Absolute Path','Relative Path','Bounding Boxes', 'Output'])","metadata":{"cell_id":"77931a5914c24c2fb7f283e17f66d72e","source_hash":"3fff1548","execution_start":1682468141934,"execution_millis":100,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"7164"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import albumentations as A        # For data augmentation\nimport cv2                        # For loading images\nimport matplotlib.pyplot as plt   # For plotting images\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclasses = {\"Pneumonia\": 0,\n            \"COVID\": 1,\n            \"Cardiomegaly\": 2}\n\nfor i in range(len(X_train)):\n    labels = []\n    bbox = []\n    dic_list = X_train[i][2]\n    dic = list(eval(dic_list))\n    for j in range(len(dic)):\n        d = dic[j]\n        x, y, w, h = d['x'], d['y'], None, None\n        if 'width' in d:\n            w = d['width']\n        if 'w' in d:\n            w = d['w']\n        if 'height' in d:\n            h = d['height']\n        if 'h' in d:\n            h = d['h']\n\n        x, y, w, h = center_format_rescale(x, y, w, h, 256, 256, 640, 640)\n        bbox.append([x, y, w, h])\n        #print(bbox)\n        #print(y_train[0])\n        labels.append(y_train[i])\n\n    transform1 = A.Compose([\n        A.HorizontalFlip(p=1.0)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    transform2 = A.Compose([\n        A.Rotate(limit=45, p=1.0)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    transform3 = A.Compose([\n        A.RandomCrop(width=640, height=640)\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n\n    image = Image.open(X_train[i][0])\n    image.save('normal.png')\n    image = np.array(image)\n    transformed1 = transform1(image=image, bboxes=bbox, class_labels=labels)\n    transformed2 = transform2(image=image, bboxes=bbox, class_labels=labels)\n    transformed3 = transform3(image=image, bboxes=bbox, class_labels=labels)\n\n    img1 = Image.fromarray(transformed1['image'])\n    img2 = Image.fromarray(transformed2['image'])\n    img3 = Image.fromarray(transformed3['image'])\n\n    rgb1 = img1.convert('RGB')\n    rgb2 = img2.convert('RGB')\n    rgb3 = img3.convert('RGB')\n\n    input_dir = '/data/Train/images/'\n    filename1 = os.path.join(input_dir, f'Horizontal{X_train[i][1]}')\n    filename2 = os.path.join(input_dir, f'Rotate{X_train[i][1]}')\n    filename3 = os.path.join(input_dir, f'Crop{X_train[i][1]}')\n    img1 = Image.fromarray(transformed1['image'])\n    img2 = Image.fromarray(transformed2['image'])\n    img3 = Image.fromarray(transformed3['image'])\n\n    if not os.path.exists(input_dir):\n        os.makedirs(input_dir)\n\n    #print(filename1)\n    img1.save(filename1)\n    img2.save(filename2)\n    img3.save(filename3)\n\n\n    # Add bounding box to output list\n    b1, b2, b3 = transformed1['bboxes'], transformed2['bboxes'], transformed3['bboxes']\n    string1 = ''\n    string2 = ''\n    string3 = ''\n    for l in range(len(b1)):\n        string1 += f\"{classes[labels[l]]} {b1[l]} {b1[l]} {b1[l]}\\n\"\n    for l in range(len(b2)):\n        string2 += f\"{classes[labels[l]]} {b2[l]} {b2[l]} {b2[l]}\\n\"\n    for l in range(len(b3)):\n        string3 += f\"{classes[labels[l]]} {b3[l]} {b3[l]} {b3[l]}\\n\"\n\n    filename = 'yolov7/data/Train/labels/' \n    if not os.path.exists(filename):\n        os.makedirs(filename)\n\n    filename1 = filename + f'Horizontal{X_train[i][1][:-4]}.txt'\n    with open(filename1, \"w\") as file:\n        file.write(string1)\n    \n    filename2 = filename + f'Rotate{X_train[i][1][:-4]}.txt'\n    with open(filename2, \"w\") as file:\n        file.write(string2)\n    \n    filename3 = filename + f'Crop{X_train[i][1][:-4]}.txt'\n    with open(filename3, \"w\") as file:\n        file.write(string3)","metadata":{"cell_id":"2ca21fd710644da2a1f5c97b1a2ea164","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  dataset to coco format\nimport numpy as np\nimport json\nimport datetime\nimport pandas as pd\n\npath = 'actualObject2.csv' # the path to the CSV file\nsave_json_path = 'custom_train.json'\n\ndata = traindf\n\nCLASSES = [\n    'Pneumonia', 'Cardiomegaly', 'COVID'\n]\n\nimages = []\ncategories = []\nannotations = []\n\ncategory = {}\ncategory[\"supercategory\"] = 'none'\ncategory[\"id\"] = 0\ncategory[\"name\"] = 'None'\ncategories.append(category)\n\ndata['fileid'] = data['Absolute Path']\ndata['categoryid'] = pd.Categorical(data['Output'],ordered= True).codes\ndata['categoryid'] = data['categoryid'] + 1\ndata['annid'] = data['Absolute Path']\n\ndef image(row):\n    image = {}\n    image[\"height\"] = 256\n    image[\"width\"] = 256\n    image[\"id\"] = row[0]\n    image[\"file_name\"] = row[2]\n    image['license'] = 0\n    image['coco_url'] = ''\n    return image\n\ndef category(row):\n    category = {}\n    category[\"supercategory\"] = \"none\"\n    category[\"id\"] = CLASSES.index(row[4]) + 1\n    category[\"name\"] = row[4]\n    return category\n\ndef annotation(row):\n    annotation = {}\n    dic = list(eval(row[3]))[0]\n    if 'w' in dic:\n        area = (int(list(eval(row[3]))[0]['w']))*(int(list(eval(row[3]))[0]['h']))\n    else:\n        area = (int(list(eval(row[3]))[0]['width']))*(int(list(eval(row[3]))[0]['height']))\n    annotation[\"segmentation\"] = []\n    annotation[\"iscrowd\"] = 0\n    annotation[\"area\"] = area\n    annotation[\"image_id\"] = row[0]\n    if ('w' in dic):\n        annotation[\"bbox\"] = [list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y'], list(eval(row[3]))[0]['w'] + list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y']+list(eval(row[3]))[0]['h']]\n    else:\n        annotation[\"bbox\"] = [list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y'], list(eval(row[3]))[0]['width'] + list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y']+list(eval(row[3]))[0]['height']]\n\n    annotation[\"category_id\"] = CLASSES.index(row[4])\n    annotation[\"id\"] = row[0]\n    return annotation\n\nfor row in data.itertuples():\n    annotations.append(annotation(row))\n\nimagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\nfor row in imagedf.itertuples():\n    images.append(image(row))\n\ncatdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\nfor row in catdf.itertuples():\n    categories.append(category(row))\n\ndata_coco = {}\ndata_coco[\"images\"] = images\ndata_coco[\"categories\"] = categories\ndata_coco[\"annotations\"] = annotations\n\n        \njson.dump(data_coco, open(save_json_path, \"w\"), indent=4)\n\nwith open('/work/custom_train.json', 'r+') as f:\n    data = json.load(f)\n    data['info'] = {\n        \"year\": \"2023\",\n        \"version\": \"1.0\",\n        \"description\": \"Exported from Kaggle and NCBI\",\n        \"contributor\": \"Kaggle, NCBI\"}\n     # <--- add `id` value.\n    f.seek(0)        # <--- should reset file position to the beginning.\n    json.dump(data, f, indent=4)\n    f.truncate()     # remove remaining part\n\nwith open('/work/custom_train.json', 'r+') as f:\n    data = json.load(f)\n    data['licenses'] = {\n          \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\n          \"id\": 1,\n          \"name\": \"Attribution-NonCommercial-ShareAlike License\"\n        }\n     # <--- add `id` value.\n    f.seek(0)        # <--- should reset file position to the beginning.\n    json.dump(data, f, indent=4)\n    f.truncate()     # remove remaining part\nprint('FINISHED CONVERTING TRAIN')","metadata":{"cell_id":"cc16a22bf2404942a866e4e5fce2985b","source_hash":"4ba491ff","output_cleared":false,"execution_start":1682468214522,"execution_millis":3219,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"FINISHED CONVERTING train\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#  dataset to coco format\nimport numpy as np\nimport json\nimport datetime\nimport pandas as pd\n\npath = 'actualObject2.csv' # the path to the CSV file\nsave_json_path = 'custom_test.json'\n\ndata = testdf\n\nCLASSES = [\n    'Pneumonia', 'Cardiomegaly', 'COVID'\n]\n\nimages = []\ncategories = []\nannotations = []\n\ncategory = {}\ncategory[\"supercategory\"] = 'none'\ncategory[\"id\"] = 0\ncategory[\"name\"] = 'None'\ncategories.append(category)\n\ndata['fileid'] = data['Absolute Path']\ndata['categoryid'] = pd.Categorical(data['Output'],ordered= True).codes\ndata['categoryid'] = data['categoryid'] + 1\ndata['annid'] = data['Absolute Path']\n\ndef image(row):\n    image = {}\n    image[\"height\"] = 256\n    image[\"width\"] = 256\n    image[\"id\"] = row[0]\n    image[\"file_name\"] = row[2]\n    image['license'] = 0\n    image['coco_url'] = ''\n    return image\n\ndef category(row):\n    category = {}\n    category[\"supercategory\"] = \"none\"\n    category[\"id\"] = CLASSES.index(row[4]) + 1\n    category[\"name\"] = row[4]\n    return category\n\ndef annotation(row):\n    annotation = {}\n    dic = list(eval(row[3]))[0]\n    if 'w' in dic:\n        area = (int(list(eval(row[3]))[0]['w']))*(int(list(eval(row[3]))[0]['h']))\n    else:\n        area = (int(list(eval(row[3]))[0]['width']))*(int(list(eval(row[3]))[0]['height']))\n    annotation[\"segmentation\"] = []\n    annotation[\"iscrowd\"] = 0\n    annotation[\"area\"] = area\n    annotation[\"image_id\"] = row[0]\n    if ('w' in dic):\n        annotation[\"bbox\"] = [list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y'], list(eval(row[3]))[0]['w'] + list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y']+list(eval(row[3]))[0]['h']]\n    else:\n        annotation[\"bbox\"] = [list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y'], list(eval(row[3]))[0]['width'] + list(eval(row[3]))[0]['x'], list(eval(row[3]))[0]['y']+list(eval(row[3]))[0]['height']]\n\n    annotation[\"category_id\"] = CLASSES.index(row[4])\n    annotation[\"id\"] = row[0]\n    return annotation\n\nfor row in data.itertuples():\n    annotations.append(annotation(row))\n\nimagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\nfor row in imagedf.itertuples():\n    images.append(image(row))\n\ncatdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\nfor row in catdf.itertuples():\n    categories.append(category(row))\n\ndata_coco = {}\ndata_coco[\"images\"] = images\ndata_coco[\"categories\"] = categories\ndata_coco[\"annotations\"] = annotations\n\n        \njson.dump(data_coco, open(save_json_path, \"w\"), indent=4)\n\nwith open('/work/custom_test.json', 'r+') as f:\n    data = json.load(f)\n    data['info'] = {\n        \"year\": \"2023\",\n        \"version\": \"1.0\",\n        \"description\": \"Exported from Kaggle and NCBI\",\n        \"contributor\": \"Kaggle, NCBI\"}\n     # <--- add `id` value.\n    f.seek(0)        # <--- should reset file position to the beginning.\n    json.dump(data, f, indent=4)\n    f.truncate()     # remove remaining part\n\nwith open('/work/custom_test.json', 'r+') as f:\n    data = json.load(f)\n    data['licenses'] = {\n          \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\",\n          \"id\": 1,\n          \"name\": \"Attribution-NonCommercial-ShareAlike License\"\n        }\n     # <--- add `id` value.\n    f.seek(0)        # <--- should reset file position to the beginning.\n    json.dump(data, f, indent=4)\n    f.truncate()     # remove remaining part\nprint('FINISHED CONVERTING TEST')","metadata":{"cell_id":"0a7c6fd38d524b2aaac478ee15d99361","source_hash":"fc83824","execution_start":1682468218103,"execution_millis":563,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"FINISHED CONVERTING TEST\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\npretrained = True\n\nif pretrained:\n    # Get pretrained weights\n    checkpoint = torch.hub.load_state_dict_from_url(\n                url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n                map_location='cpu',\n                check_hash=True)\n\n    # Remove class weights\n    del checkpoint[\"model\"][\"class_embed.weight\"]\n    del checkpoint[\"model\"][\"class_embed.bias\"]\n\n    # SaveOGH\n    torch.save(checkpoint,\n               'detr-r50_no-class-head.pth')","metadata":{"cell_id":"685ced8bf0fd48e3afa58bfc8d37eaef","source_hash":"4623c398","output_cleared":false,"execution_start":1682468237250,"execution_millis":2387,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torchvision\nimport os\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, processor, train=True):\n        ann_file = os.path.join(img_folder, \"custom_train.json\" if train else \"custom_val.json\")\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self.processor = processor\n\n    def __getitem__(self, idx):\n        # read in PIL image and target in COCO format\n        # feel free to add data augmentation here before passing them to the next step\n   \n        img, target = super(CocoDetection, self).__getitem__(idx)\n        \n        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n        target = encoding[\"labels\"][0] # remove batch dimension\n\n        return pixel_values, target","metadata":{"cell_id":"91bdac7295d14bfb8857eb585c290bd9","source_hash":"2337804d","output_cleared":false,"execution_start":1682468298367,"execution_millis":530,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DetrImageProcessor\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n\ntrain_dataset = CocoDetection(img_folder='/work/cocoData/Train', processor=processor)\nval_dataset = CocoDetection(img_folder='/work/cocoData/Test', processor=processor, train=False)\n     ","metadata":{"cell_id":"a918cab71ac54e08999cad03218da92a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(val_dataset))","metadata":{"cell_id":"dfa7f092156e47b7b7cf7b85a1076619","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom PIL import Image, ImageDraw\n\n# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\nimage_ids = train_dataset.coco.getImgIds()\n# let's pick a random image\nimage_id = image_ids[np.random.randint(0, len(image_ids))]\nprint('Image n°{}'.format(image_id))\nimage = train_dataset.coco.loadImgs(image_id)[0]\nimage = Image.open(os.path.join('/work/cocoData/Train', image['file_name']))\n\nannotations = train_dataset.coco.imgToAnns[image_id]\n\nnew_image = image.convert('RGB')\nprint(new_image.size)\nprintz()\ndraw = ImageDraw.Draw(new_image)\n\ncats = train_dataset.coco.cats\nid2label = {k: v['name'] for k,v in cats.items()}\n\nfor annotation in annotations:\n  box = annotation['bbox']\n  class_idx = annotation['category_id']\n  x,y,w,h = tuple(box)\n  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n  draw.text((x, y), id2label[class_idx], fill='white')\n\nnew_image.show()","metadata":{"cell_id":"b06d41aa98334e348caaed8800a34fb6","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  pixel_values = [item[0] for item in batch]\n  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n  labels = [item[1] for item in batch]\n  batch = {}\n  batch['pixel_values'] = encoding['pixel_values']\n  batch['pixel_mask'] = encoding['pixel_mask']\n  batch['labels'] = labels\n  return batch\n\ntrain_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)\nbatch = next(iter(train_dataloader))","metadata":{"cell_id":"cedc23efeda34609ab8acf825a590d0f","source_hash":"a5b2045d","output_cleared":false,"execution_start":1682468395939,"execution_millis":159,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/root/venv/lib/python3.9/site-packages/transformers/models/detr/image_processing_detr.py:886: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from transformers import DetrConfig, DetrForObjectDetection\nimport pytorch_lightning as pl\nimport torch\n\nclass Detr(pl.LightningModule):\n     def __init__(self, lr, lr_backbone, weight_decay):\n         super().__init__()\n         # replace COCO classification head with custom head\n         # we specify the \"no_timm\" variant here to not rely on the timm library\n         # for the convolutional backbone\n         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n                                                             revision=\"no_timm\", \n                                                             num_labels=len(id2label),\n                                                             ignore_mismatched_sizes=True)\n         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n         self.lr = lr\n         self.lr_backbone = lr_backbone\n         self.weight_decay = weight_decay\n\n     def forward(self, pixel_values, pixel_mask):\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n       return outputs\n     \n     def common_step(self, batch, batch_idx):\n       pixel_values = batch[\"pixel_values\"]\n       pixel_mask = batch[\"pixel_mask\"]\n       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n\n       loss = outputs.loss\n       loss_dict = outputs.loss_dict\n\n       return loss, loss_dict\n\n     def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"train_\" + k, v.item())\n\n        return loss\n\n     def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        self.log(\"validation_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"validation_\" + k, v.item())\n\n        return loss\n\n     def configure_optimizers(self):\n        param_dicts = [\n              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n              {\n                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n                  \"lr\": self.lr_backbone,\n              },\n        ]\n        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n                                  weight_decay=self.weight_decay)\n        \n        return optimizer\n\n     def train_dataloader(self):\n        return train_dataloader\n\n     def val_dataloader(self):\n        return val_dataloader","metadata":{"cell_id":"cc84e264bf384c4d828e88d6fb2b1029","source_hash":"b48dc432","output_cleared":false,"execution_start":1682468399158,"execution_millis":2153,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start tensorboard.\n%load_ext tensorboard\n%tensorboard --logdir lightning_logs/\n     ","metadata":{"cell_id":"9938fef7f05245fe961e7fe18ad81ee3","source_hash":"67cdd0a8","output_cleared":false,"execution_start":1682468402967,"execution_millis":11120,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":"Launching TensorBoard..."},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"cell_type":"code","source":"model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])","metadata":{"cell_id":"318ef9a42cf3430fa6d4b36bb7f08216","source_hash":"2eac2901","output_cleared":false,"execution_start":1682468414089,"execution_millis":12065,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([5, 256]) in the model instantiated\n- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"cell_id":"05d93027b89246aaaf255894fbce6ee5","source_hash":"d64527a5","execution_start":1682468426107,"execution_millis":49,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"torch.Size([4, 100, 5])"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\n\ntrainer = Trainer(max_steps=300, gradient_clip_val=0.1)\ntrainer.fit(model)","metadata":{"cell_id":"57883fd8a0914a4e811cecd5d9b7c8cd","source_hash":"398f979d","execution_start":1682468428496,"execution_millis":220213,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type                   | Params\n-------------------------------------------------\n0 | model | DetrForObjectDetection | 41.5 M\n-------------------------------------------------\n18.0 M    Trainable params\n23.5 M    Non-trainable params\n41.5 M    Total params\n166.010   Total estimated model params size (MB)\nSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]/root/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/root/venv/lib/python3.9/site-packages/transformers/models/detr/image_processing_detr.py:886: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n  warnings.warn(\nSanity Checking DataLoader 0:  50%|█████     | 1/2 [00:04<00:04,  4.98s/it]/root/venv/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n  warning_cache.warn(\n                                                                           /root/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\nEpoch 0:   1%|          | 17/1767 [03:27<5:56:08, 12.21s/it, v_num=1]/root/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8ce3b300-df7c-425c-8b8b-e9e441a56a3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"0424886eb04b445484072501c853e224","deepnote_persisted_session":{"createdAt":"2023-04-28T18:51:08.004Z"},"deepnote_execution_queue":[]}}